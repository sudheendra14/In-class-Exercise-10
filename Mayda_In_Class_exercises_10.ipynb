{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Mayda In_Class_exercises-10.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZqsyhQwhZV"
      },
      "source": [
        "# In class exercise 10 (20 points in total, 4/16/2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gIsPTu0whZ9"
      },
      "source": [
        "The purpose of the exercise is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K means, \n",
        "DBSCAN,\n",
        "Hierarchical clustering. \n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcdMDB36whaE"
      },
      "source": [
        "#Write your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkd7g_5JwhaI",
        "outputId": "77a7186e-eb84-46e6-8d12-a6a539f710bc"
      },
      "source": [
        "#You can write you answer here.\n",
        "#Write your code here.\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from textblob import Word\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import gensim\n",
        "import scipy\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Jv812mVKzUDF",
        "outputId": "9154f47a-9088-43cc-916a-f39cf318eaea"
      },
      "source": [
        "mydata=pd.read_csv(\"/content/Amazon_Unlocked_Mobile.csv\")\n",
        "mydata.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Brand Name</th>\n",
              "      <th>Price</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Review Votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>5</td>\n",
              "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>5</td>\n",
              "      <td>Very pleased</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>It works good but it goes slow sometimes but i...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>Great phone to replace my lost phone. The only...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Product Name  ... Review Votes\n",
              "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          1.0\n",
              "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJvc6Ou20GdW",
        "outputId": "24f86bb0-1ea3-4652-a30d-458ebba8bfe8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CyOUl900GXQ"
      },
      "source": [
        "mydata['CleanedReviews'] = mydata['Reviews'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "mydata['CleanedReviews'] = mydata['CleanedReviews'].str.replace('[^\\w\\s]','')\n",
        "stop = stopwords.words('english')\n",
        "mydata['CleanedReviews'] = mydata['CleanedReviews'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "mydata['CleanedReviews'] = mydata['CleanedReviews'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(mydata['CleanedReviews'].values)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25jrvOJx0Qom"
      },
      "source": [
        "number_of_clusters = [x for x in range(3, 10)]\n",
        "squared_errors = []\n",
        "for cluster in number_of_clusters:\n",
        "    kmeans = KMeans(n_clusters = cluster).fit(tfidf_vectors)\n",
        "    squared_errors.append(kmeans.inertia_)\n",
        "optimal_clusters = np.argmin(squared_errors) + 2 \n",
        "plt.plot(number_of_clusters , squared_errors)\n",
        "plt.title(\"Finding the number of clusters using Elbow Method.\")\n",
        "plt.xlabel(\"clusters.\")\n",
        "plt.ylabel(\"Loss.\")\n",
        "xy = (optimal_clusters, min(squared_errors))\n",
        "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
        "plt.show()\n",
        "print (\"The optimal number of clusters obtained is - \", optimal_clusters)\n",
        "print (\"The loss for optimal cluster is - \", min(squared_errors))\n",
        "optimal_k = 5\n",
        "model = KMeans(n_clusters = optimal_k).fit(tfidf_vectors)\n",
        "labels = model.labels_\n",
        "cluster_centers = model.cluster_centers_\n",
        "terms = tfidf_vectorizer.get_feature_names()\n",
        "silhouette_score = metrics.silhouette_score(tfidf_vectors, labels, metric='euclidean')\n",
        "print('Silhouette Score: ', silhouette_score)\n",
        "mydata['TF-IDF Cluster Labels'] = labels\n",
        "print(mydata.groupby(['TF-IDF Cluster Labels'])['Reviews'].count())\n",
        "print(\"Highest terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(optimal_k):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i, :optimal_k]:\n",
        "        print(' %s' % terms[ind], end='')\n",
        "        print()\n",
        "plt.bar([x for x in range(optimal_k)], mydata.groupby(['TF-IDF Cluster Labels'])['Reviews'].count(), alpha = 0.4)\n",
        "plt.title('KMeans cluster')\n",
        "plt.xlabel(\"Cluster_number\")\n",
        "plt.ylabel(\"Number_points\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyDjvzTB0f1R"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "list_of_reviews=[]\n",
        "for review in mydata['CleanedReviews'].values:\n",
        "    list_of_reviews.append(review.split())\n",
        "w2v_model=gensim.models.Word2Vec(list_of_reviews, size=100, workers=4)\n",
        "\n",
        "review_vectors = []\n",
        "for review in list_of_reviews:\n",
        "    review_vec = np.zeros(100)\n",
        "    count_words = 0\n",
        "    for word in review:\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            review_vec += vec\n",
        "            count_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    review_vec /= count_words\n",
        "    review_vectors.append(review_vec)\n",
        "    \n",
        "review_vectors = np.array(review_vectors)\n",
        "review_vectors = np.nan_to_num(review_vectors)\n",
        "minPts = 1 * 100\n",
        "def lower_bound(nums, target):\n",
        "    l, r = 0, len(nums) - 1\n",
        "    while l <= r: \n",
        "        mid = int(l + (r - l) / 2)\n",
        "        if nums[mid] >= target:\n",
        "            r = mid - 1\n",
        "        else:\n",
        "            l = mid + 1\n",
        "    return l\n",
        "def compute_100th_nearest_neighbour(x, data): \n",
        "    dists = []\n",
        "    for val in data:\n",
        "        dist = np.sum((x - val) **1 )\n",
        "        if(len(dists) == 100 and dists[99] > dist): \n",
        "            l = int(lower_bound(dists, dist))\n",
        "            if l < 100 and l >= 0 and dists[l] > dist:\n",
        "                dists[l] = dist\n",
        "        else:\n",
        "            dists.append(dist)\n",
        "            dists.sort()\n",
        "    \n",
        "    return dists[99]\n",
        "\n",
        "one_hundreth_neighbour = []\n",
        "for val in review_vectors[:1500]:\n",
        "    one_hundreth_neighbour.append(compute_100th_nearest_neighbour(val, review_vectors[:1500]) )\n",
        "one_hundreth_neighbour.sort()\n",
        "plt.figure(figsize=(10,2))\n",
        "plt.plot([x for x in range(len(one_hundreth_neighbour))], one_hundreth_neighbour)\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Distance of 100th Nearest Neighbour\")\n",
        "plt.show()\n",
        "optimal_epsilon = 3\n",
        "dbscan = DBSCAN(eps = optimal_epsilon, min_samples = 100).fit(review_vectors)\n",
        "mydata['DBSCAN Cluster Labels'] = dbscan.labels_\n",
        "print(mydata.groupby(['DBSCAN Cluster Labels'])['Reviews'].count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trvx-E-j0hr2"
      },
      "source": [
        "dendro = hierarchy.dendrogram(hierarchy.linkage(review_vectors, method='ward'))\n",
        "plt.axhline(y=20)\n",
        "number_of_clusters = 3\n",
        "agg_model = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
        "Agg = agg_model.fit_predict(review_vectors)\n",
        "h_labels = agg_model.labels_\n",
        "h_silhouette_score = metrics.silhouette_score(review_vectors, h_labels, metric='euclidean')\n",
        "print('Silhouette Score is: ', h_silhouette_score)\n",
        "mydata['Hierarchical Cluster Labels'] = h_labels\n",
        "mydata.groupby(['Hierarchical Cluster Labels'])['Reviews'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iHYsQIxwhaH"
      },
      "source": [
        "In one paragraph, please compare K means, DBSCAN and Hierarchical clustering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fws4DSfO1Kcx"
      },
      "source": [
        "#You can write you answer here.\n",
        "To find the optimal number of clusters for k-means, as well as the optimal number of epsilons for DBSCAN clustering and dendogram for Hierarchial clustering, I used the elbow approach discussed in class.\n",
        "DBSCAN, based on the above study, appears to be the most efficient.\n",
        "\n",
        "K-means Clustering is the second best because it has a higher silhouette ranking, led by hierarchical doing not too poorly."
      ]
    }
  ]
}